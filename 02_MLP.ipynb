{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02. MLP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youuRee/DeepLearning_Lecture/blob/main/02_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJT5kqyZGe1p"
      },
      "source": [
        "# Multi-layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzHP2rndF-0G"
      },
      "source": [
        "## 외부 파일 가져오기 & requirements 설치"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/#fastcampus\")\n",
        "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
      ],
      "metadata": {
        "id": "PEL1W9qn4FtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD7vyST9GdHv"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX3soatAMk5B"
      },
      "source": [
        "from data_utils import dataset_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform\n",
        "- transforms.ToTensor() : 데이터를 tensor로 바꿔준다.\n",
        "-transforms.Normalize(mean, std, inplace=False) : 정규화한다.\n",
        "-transforms.ToPILImage() : csv 파일로 데이터셋을 받을 경우, PIL image로 바꿔준다.\n",
        "- transforms.Compose : 여러 단계로 변환해야 하는 경우, Compose를 통해 여러 단계를 묶을 수 있다"
      ],
      "metadata": {
        "id": "smRnwNYbCbJL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lISGUqfUG_Ny"
      },
      "source": [
        "data_root = os.path.join(os.getcwd(), \"data\")\n",
        "\n",
        "# 전처리 부분 (preprocessing) & 데이터 셋 정의.\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]), # mean, # std\n",
        "    ]\n",
        ")\n",
        "fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0waoPp23NcE0"
      },
      "source": [
        "## Dataloader를 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Nq1tZiJ84g"
      },
      "source": [
        "# 훈련데이터 : 0.9, 테스트데이터: 0.1 비율로 나눔\n",
        "datasets = dataset_split(fashion_mnist_dataset, split=[0.9, 0.1])\n",
        "\n",
        "train_dataset = datasets[\"train\"]\n",
        "val_dataset = datasets[\"val\"]\n",
        "\n",
        "train_batch_size = 100\n",
        "val_batch_size = 10\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1\n",
        ")\n",
        "# num_workers : 병렬 프로세싱할 때 얼마나 있으면 좋냐\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRCanL6LtoM"
      },
      "source": [
        "for sample_batch in train_dataloader:\n",
        "    print(sample_batch)\n",
        "    print(sample_batch[0].shape, sample_batch[1].shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwd5nW_3N37z"
      },
      "source": [
        "## 모델 (Multi-layer Perceptron) (MLP) ! 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O--VMLMN04F"
      },
      "source": [
        "# Define Model.\n",
        "# nn.Module 사용 --> 사용안하면 디바이스 변경시 꼬임\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_dim, h1_dim)\n",
        "        self.linear2 = nn.Linear(h1_dim, h2_dim)\n",
        "        self.linear3 = nn.Linear(h2_dim, out_dim)\n",
        "        self.relu = F.relu\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x = torch.flatten(input, start_dim=1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        out = self.linear3(x)\n",
        "        # out = F.softmax(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8tljBSaQBZB"
      },
      "source": [
        "## 모델 선언 및 손실 함수, 최적화(Optimizer) 정의, Tensorboard Logger 정의 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8p_hsjsO-on"
      },
      "source": [
        "# define model.\n",
        "model = MLP(28*28, 128, 64, 10)\n",
        "\n",
        "# define loss\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# define optimizer\n",
        "#  parameters() 메소드는 먼저 모든 하위 모듈들을 탐색하고(recursive=True), 각 모듈의 _parameters에 들어있는 파라미터들을 하나씩 반환해주는 함수\n",
        "# 모델 a->b->c , a.parameter() : c의 파라미터 반환\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "max_epoch = 15\n",
        "\n",
        "# define tensorboard logger\n",
        "writer = SummaryWriter()\n",
        "log_interval = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq-XpcGXQu1v"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/\n",
        "\n",
        "train_step = 0\n",
        "for epoch in range(1, max_epoch+1):\n",
        "    # valid step\n",
        "    # valid data에 최적화x --> no_grad()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        for val_batch_idx, (val_images, val_labels) in enumerate(\n",
        "            tqdm(val_dataloader, position=0, leave=True, desc=\"validation\")\n",
        "        ):\n",
        "            # forward\n",
        "            val_outputs = model(val_images)\n",
        "            _, val_preds = torch.max(val_outputs, 1)\n",
        "            \n",
        "            # loss & acc\n",
        "            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0]\n",
        "            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n",
        "    \n",
        "    # valid step logging\n",
        "    val_epoch_loss = val_loss / len(val_dataloader)\n",
        "    val_epoch_acc = val_corrects / len(val_dataloader)\n",
        "    \n",
        "    print(\n",
        "        f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\"\n",
        "    )\n",
        "    writer.add_scalar(\"Loss/val\", val_epoch_loss, train_step)\n",
        "    writer.add_scalar(\"Acc/val\", val_epoch_acc, train_step)\n",
        "    writer.add_images(\"Images/val\", val_images, train_step)\n",
        "    \n",
        "    # train step\n",
        "    current_loss = 0\n",
        "    current_corrects = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(\n",
        "         tqdm(train_dataloader, position=0, leave=True, desc=\"training\")\n",
        "    ):\n",
        "        current_loss = 0.0\n",
        "        current_corrects = 0\n",
        "\n",
        "        # Forward\n",
        "        # get predictions\n",
        "        outputs = model(images)\n",
        "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        # get loss (Loss 계산)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        # optimizer 초기화 (zero화)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform Optimization\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        current_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if train_step % log_interval == 0:\n",
        "            train_loss = current_loss / log_interval\n",
        "            train_acc = current_corrects / log_interval\n",
        "\n",
        "            print(\n",
        "                f\"{train_step}: train_loss: {train_loss}, train_acc: {train_acc}\"\n",
        "            )\n",
        "            writer.add_scalar(\"Loss/train\", train_loss, train_step)\n",
        "            writer.add_scalar(\"Acc/train\", train_acc, train_step)\n",
        "            writer.add_images(\"Images/train\", images, train_step)\n",
        "            writer.add_graph(model, images)\n",
        "            current_loss = 0\n",
        "            current_corrects = 0\n",
        "\n",
        "        train_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1oxnXSyQ2b7"
      },
      "source": [
        "# save model\n",
        "os.makedirs(\"./logs/models\", exist_ok=True)\n",
        "torch.save(model, \"./logs/models/mlp.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymnDN1VkYOvt"
      },
      "source": [
        "# load model\n",
        "loaded_model = torch.load(\"./logs/models/mlp.ckpt\")\n",
        "loaded_model.eval()\n",
        "print(loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvB9xAnYWBk"
      },
      "source": [
        "def softmax(x, axis=0):\n",
        "    \"numpy softmax\" # numpy 형태로 저장\n",
        "    max = np.max(x, axis=axis, keepdims=True)\n",
        "    e_x = np.exp(x - max)\n",
        "    sum = np.sum(e_x, axis=axis, keepdims=True)\n",
        "    f_x = e_x / sum\n",
        "    return f_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UoTf2ztY7nT"
      },
      "source": [
        "# Test\n",
        "test_batch_size = 100\n",
        "test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transforms.ToTensor())\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "test_labels_list = []\n",
        "test_preds_list = []\n",
        "test_outputs_list = []\n",
        "\n",
        "for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n",
        "    # forward\n",
        "    test_outputs = loaded_model(test_images)\n",
        "    _, test_preds = torch.max(test_outputs, 1)\n",
        "\n",
        "    # 분석 -> numpy 사용하는게 좋음\n",
        "    # GPU -> CPU detach()\n",
        "    final_outs = softmax(test_outputs.detach().numpy(), axis=1)\n",
        "    test_outputs_list.extend(final_outs)\n",
        "    test_preds_list.extend(test_preds.detach().numpy())\n",
        "    test_labels_list.extend(test_labels.detach().numpy())\n",
        "\n",
        "test_preds_list = np.array(test_preds_list)\n",
        "test_labels_list = np.array(test_labels_list)\n",
        "\n",
        "print(f\"\\nacc: {np.mean(test_preds_list == test_labels_list)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0a4PPefaRfH"
      },
      "source": [
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh = {}\n",
        "n_class = 10\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)\n",
        "\n",
        "# plot.\n",
        "for i in range(n_class):\n",
        "    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n",
        "plt.title(\"Multi-class ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "print(\"auc_score\", roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm5jZ7urbMAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}