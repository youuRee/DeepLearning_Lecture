{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03. MLP + basic regularizers (dropout, early stopping).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youuRee/DeepLearning_Lecture/blob/main/Part5/03_MLP_%2B_basic_regularizers_(dropout%2C_early_stopping).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJT5kqyZGe1p"
      },
      "source": [
        "# Multi-layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzHP2rndF-0G"
      },
      "source": [
        "## 외부 파일 가져오기 & requirements 설치"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "sys.path.append(\"/content/drive/MyDrive/#fastcampus\")\n",
        "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
      ],
      "metadata": {
        "id": "jBkoWxTN64-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD7vyST9GdHv"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX3soatAMk5B"
      },
      "source": [
        "from data_utils import dataset_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lISGUqfUG_Ny"
      },
      "source": [
        "data_root = os.path.join(os.getcwd(), \"data\")\n",
        "\n",
        "# 전처리 부분 (preprocessing) & 데이터 셋 정의.\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]), # mean, # std\n",
        "    ]\n",
        ")\n",
        "fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0waoPp23NcE0"
      },
      "source": [
        "## Dataloader를 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Nq1tZiJ84g"
      },
      "source": [
        "datasets = dataset_split(fashion_mnist_dataset, split=[0.9, 0.1])\n",
        "\n",
        "train_dataset = datasets[\"train\"]\n",
        "val_dataset = datasets[\"val\"]\n",
        "\n",
        "train_batch_size = 100\n",
        "val_batch_size = 10\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRCanL6LtoM"
      },
      "source": [
        "for sample_batch in train_dataloader:\n",
        "    print(sample_batch)\n",
        "    print(sample_batch[0].shape, sample_batch[1].shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwd5nW_3N37z"
      },
      "source": [
        "## 모델 (Multi-layer Perceptron) (MLP) ! 정의\n",
        "## 모델 MLPWithDropout 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O--VMLMN04F"
      },
      "source": [
        "# Define Model.\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_dim, h1_dim)\n",
        "        self.linear2 = nn.Linear(h1_dim, h2_dim)\n",
        "        self.linear3 = nn.Linear(h2_dim, out_dim)\n",
        "        self.relu = F.relu\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = torch.flatten(input, start_dim=1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        out = self.linear3(x)\n",
        "        # out = F.softmax(out)\n",
        "        return out\n",
        "\n",
        "class MLPWithDropout(MLP):\n",
        "    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int, dropout_prob: float):\n",
        "        super().__init__(in_dim, h1_dim, h2_dim, out_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = torch.flatten(input, start_dim=1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.dropout2(x)\n",
        "        out = self.linear3(x)\n",
        "        # out = F.softmax(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8tljBSaQBZB"
      },
      "source": [
        "## 모델 선언 및 손실 함수, 최적화(Optimizer) 정의, Tensorboard Logger 정의 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8p_hsjsO-on"
      },
      "source": [
        "# define model.\n",
        "# model = MLP(28*28, 128, 64, 10)\n",
        "model = MLPWithDropout(28*28, 128, 64, 10, dropout_prob=0.3)\n",
        "model_name = type(model).__name__ # type이 model인 class 이름 가져옴(참조) --> MLPWithDropout\n",
        "\n",
        "# define loss\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "max_epoch = 50\n",
        "\n",
        "# define tensorboard logger\n",
        "# runs폴더 안에 {datetime.now()}-{model_name} 이름의 폴더 만들기\n",
        "# f \" \" 안에 있으면 그 함수 call 할 수 있음\n",
        "log_dir = f\"runs/{datetime.now()}-{model_name}\"\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "log_interval = 100\n",
        "\n",
        "# set save model path\n",
        "log_model_path = os.path.join(log_dir, \"models\") # log_dir안에 models로 경로 합침\n",
        "os.makedirs(log_model_path, exist_ok=True) # log_model_path 없으면 만들기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnGxA90KyyUi"
      },
      "source": [
        "## Early Stopping callback Object Class 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyuzST93ynLE"
      },
      "source": [
        "# With some modifications, source is from https://github.com/Bjarten/early-stopping-pytorch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.ckpt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.ckpt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        \n",
        "        filename = self.path.split('/')[-1]\n",
        "        save_dir = os.path.dirname(self.path)\n",
        "        torch.save(model, os.path.join(save_dir, f\"val_loss-{val_loss}-{filename}\"))\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq-XpcGXQu1v"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/\n",
        "\n",
        "# define EarlyStopping.\n",
        "early_stopper = EarlyStopping(\n",
        "    patience=3, verbose=True, path=os.path.join(log_model_path, \"model.ckpt\")\n",
        ")\n",
        "\n",
        "# do train with validation.\n",
        "train_step = 0\n",
        "for epoch in range(1, max_epoch+1):\n",
        "    # valid step\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
        "        # eval/val 작업이 끝난 후에는 잊지말고 train mode로 모델을 변경 --> model.train()\n",
        "        model.eval()\n",
        "\n",
        "        for val_batch_idx, (val_images, val_labels) in enumerate(\n",
        "            tqdm(val_dataloader, position=0, leave=True, desc=\"validation\")\n",
        "        ):\n",
        "            # forward\n",
        "            val_outputs = model(val_images)\n",
        "            _, val_preds = torch.max(val_outputs, 1)\n",
        "            \n",
        "            # loss & acc\n",
        "            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0]\n",
        "            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n",
        "    \n",
        "    # valid step logging\n",
        "    val_epoch_loss = val_loss / len(val_dataloader)\n",
        "    val_epoch_acc = val_corrects / len(val_dataloader)\n",
        "    \n",
        "    print(\n",
        "        f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\"\n",
        "    )\n",
        "    writer.add_scalar(\"Loss/val\", val_epoch_loss, train_step)\n",
        "    writer.add_scalar(\"Acc/val\", val_epoch_acc, train_step)\n",
        "    writer.add_images(\"Images/val\", val_images, train_step)\n",
        "\n",
        "    # check model early stopping point & save model if the model reached the best performance.\n",
        "    early_stopper(val_epoch_loss, model)\n",
        "    if early_stopper.early_stop:\n",
        "        break\n",
        "    \n",
        "    # train step\n",
        "    current_loss = 0\n",
        "    current_corrects = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(\n",
        "         tqdm(train_dataloader, position=0, leave=True, desc=\"training\")\n",
        "    ):\n",
        "        current_loss = 0.0\n",
        "        current_corrects = 0\n",
        "\n",
        "        # Forward\n",
        "        # get predictions\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        # get loss (Loss 계산)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        # optimizer 초기화 (zero화)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform Optimization\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        current_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if train_step % log_interval == 0:\n",
        "            train_loss = current_loss / log_interval\n",
        "            train_acc = current_corrects / log_interval\n",
        "\n",
        "            print(\n",
        "                f\"{train_step}: train_loss: {train_loss}, train_acc: {train_acc}\"\n",
        "            )\n",
        "            writer.add_scalar(\"Loss/train\", train_loss, train_step)\n",
        "            writer.add_scalar(\"Acc/train\", train_acc, train_step)\n",
        "            writer.add_images(\"Images/train\", images, train_step)\n",
        "            writer.add_graph(model, images)\n",
        "            current_loss = 0\n",
        "            current_corrects = 0\n",
        "\n",
        "        train_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1oxnXSyQ2b7"
      },
      "source": [
        "# save model\n",
        "# torch.save(model, os.path.join(log_model_path, \"model.ckpt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hipq3Su12nB5"
      },
      "source": [
        "log_model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymnDN1VkYOvt"
      },
      "source": [
        "# load model\n",
        "loaded_model = torch.load(os.path.join(log_model_path, \"val_loss-0.031837210059165955-model.ckpt\"))\n",
        "loaded_model.eval()\n",
        "print(loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvB9xAnYWBk"
      },
      "source": [
        "def softmax(x, axis=0):\n",
        "    \"numpy softmax\"\n",
        "    max = np.max(x, axis=axis, keepdims=True)\n",
        "    e_x = np.exp(x - max)\n",
        "    sum = np.sum(e_x, axis=axis, keepdims=True)\n",
        "    f_x = e_x / sum\n",
        "    return f_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UoTf2ztY7nT"
      },
      "source": [
        "test_batch_size = 100\n",
        "test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transforms.ToTensor())\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "test_labels_list = []\n",
        "test_preds_list = []\n",
        "test_outputs_list = []\n",
        "for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n",
        "    # forward\n",
        "    test_outputs = loaded_model(test_images)\n",
        "    _, test_preds = torch.max(test_outputs, 1)\n",
        "\n",
        "    final_outs = softmax(test_outputs.detach().numpy(), axis=1)\n",
        "    test_outputs_list.extend(final_outs)\n",
        "    test_preds_list.extend(test_preds.detach().numpy())\n",
        "    test_labels_list.extend(test_labels.detach().numpy())\n",
        "\n",
        "test_preds_list = np.array(test_preds_list)\n",
        "test_labels_list = np.array(test_labels_list)\n",
        "\n",
        "print(f\"\\nacc: {np.mean(test_preds_list == test_labels_list)*100}%\") # 그냥 MLP보다 test 성능 좋게 나옴"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0a4PPefaRfH"
      },
      "source": [
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh = {}\n",
        "n_class = 10\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)\n",
        "\n",
        "# plot.\n",
        "for i in range(n_class):\n",
        "    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n",
        "plt.title(\"Multi-class ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "print(\"auc_score\", roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm5jZ7urbMAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}